# interview-assistant

## Language model configuration

Set the `LLM_PROVIDER` environment variable to choose between the default open
source model or OpenAI:

```bash
# Default: open source model hosted on Hugging Face
export LLM_PROVIDER=open_source
export HUGGINGFACEHUB_API_TOKEN=hf_your_token

# Optional: switch to OpenAI
export LLM_PROVIDER=openai
export OPENAI_API_KEY=sk-...
```

Additional knobs:

| Variable | Description | Default |
| --- | --- | --- |
| `OPEN_SOURCE_MODEL` | Hugging Face repository for the model. | `mistralai/Mistral-7B-Instruct-v0.2` |
| `OPENAI_MODEL`/`LLM_MODEL` | OpenAI chat model name. | `gpt-3.5-turbo` |
| `LLM_MAX_NEW_TOKENS` | Maximum tokens generated by Hugging Face endpoint. | `512` |

See `config/config_setup.py` for the complete list of configuration values.
